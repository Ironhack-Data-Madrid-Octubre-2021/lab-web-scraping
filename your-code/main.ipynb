{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "from urllib.request import urlopen\n",
    "# import random\n",
    "import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "tags = soup.find_all('h1', {'class': 'h3 lh-condensed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_nombres = [tag.getText().strip() for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['文翼',\n",
       " 'angus croll',\n",
       " 'LoveSy',\n",
       " 'bokuweb',\n",
       " 'Ondřej Surý',\n",
       " 'Han Xiao',\n",
       " 'Jeffrey Su',\n",
       " 'Taner Şener',\n",
       " 'Dody Gunawinata',\n",
       " 'Matthias Fey',\n",
       " 'Matt Layher',\n",
       " 'Jimmy Wärting',\n",
       " 'Lion - dapplion',\n",
       " 'Samuel Colvin',\n",
       " 'vincent d warmerdam',\n",
       " 'Pedro Piñera Buendía',\n",
       " 'Jesse Hills',\n",
       " 'Puru Vijay',\n",
       " 'David Tolnay',\n",
       " 'Drew Powers',\n",
       " 'John Sundell',\n",
       " 'Mr.doob',\n",
       " 'Robert Mosolgo',\n",
       " 'Keegan Carruthers-Smith',\n",
       " 'Mike Cao']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags2 = soup.find_all('p', {'class': 'f4 text-normal mb-1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_usuarios = [tag.getText().strip() for tag in tags2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wenzhixin',\n",
       " 'angus-c',\n",
       " 'yujincheng08',\n",
       " 'bokuweb',\n",
       " 'oerdnj',\n",
       " 'hanxiao',\n",
       " 'JeffreySu',\n",
       " 'tanersener',\n",
       " 'dodyg',\n",
       " 'rusty1s',\n",
       " 'mdlayher',\n",
       " 'jimmywarting',\n",
       " 'dapplion',\n",
       " 'samuelcolvin',\n",
       " 'koaning',\n",
       " 'pepibumur',\n",
       " 'jesserockz',\n",
       " 'PuruVJ',\n",
       " 'dtolnay',\n",
       " 'drwpow',\n",
       " 'JohnSundell',\n",
       " 'mrdoob',\n",
       " 'rmosolgo',\n",
       " 'keegancsmith',\n",
       " 'mikecao']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(nombres, usuarios):\n",
    "    x = []\n",
    "    for i in range(len(nombres)):\n",
    "        x.append(nombres[i] + ' ' + f\"({usuarios[i]})\")\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['文翼 (wenzhixin)',\n",
       " 'angus croll (angus-c)',\n",
       " 'LoveSy (yujincheng08)',\n",
       " 'bokuweb (bokuweb)',\n",
       " 'Ondřej Surý (oerdnj)',\n",
       " 'Han Xiao (hanxiao)',\n",
       " 'Jeffrey Su (JeffreySu)',\n",
       " 'Taner Şener (tanersener)',\n",
       " 'Dody Gunawinata (dodyg)',\n",
       " 'Matthias Fey (rusty1s)',\n",
       " 'Matt Layher (mdlayher)',\n",
       " 'Jimmy Wärting (jimmywarting)',\n",
       " 'Lion - dapplion (dapplion)',\n",
       " 'Samuel Colvin (samuelcolvin)',\n",
       " 'vincent d warmerdam (koaning)',\n",
       " 'Pedro Piñera Buendía (pepibumur)',\n",
       " 'Jesse Hills (jesserockz)',\n",
       " 'Puru Vijay (PuruVJ)',\n",
       " 'David Tolnay (dtolnay)',\n",
       " 'Drew Powers (drwpow)',\n",
       " 'John Sundell (JohnSundell)',\n",
       " 'Mr.doob (mrdoob)',\n",
       " 'Robert Mosolgo (rmosolgo)',\n",
       " 'Keegan Carruthers-Smith (keegancsmith)',\n",
       " 'Mike Cao (mikecao)']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join(lista_nombres, lista_usuarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = soup.find_all('h1', {'class': 'h3 lh-condensed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_repos = [tag.getText().strip() for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python-World /\\n\\n      python-mini-projects',\n",
       " 'huggingface /\\n\\n      datasets',\n",
       " 'espnet /\\n\\n      espnet',\n",
       " 'bregman-arie /\\n\\n      devops-exercises',\n",
       " 'pytest-dev /\\n\\n      pytest',\n",
       " 'tiangolo /\\n\\n      fastapi',\n",
       " 'bhky /\\n\\n      opennsfw2',\n",
       " 'TachibanaYoshino /\\n\\n      AnimeGANv2',\n",
       " 'synacktiv /\\n\\n      CVE-2021-40539',\n",
       " 'huggingface /\\n\\n      transformers',\n",
       " 'netbox-community /\\n\\n      netbox',\n",
       " 'public-apis /\\n\\n      public-apis',\n",
       " 'WZMIAOMIAO /\\n\\n      deep-learning-for-image-processing',\n",
       " 'ansible /\\n\\n      ansible',\n",
       " 'swisskyrepo /\\n\\n      PayloadsAllTheThings',\n",
       " 'pytorch /\\n\\n      fairseq',\n",
       " 'demisto /\\n\\n      content',\n",
       " 'PyGithub /\\n\\n      PyGithub',\n",
       " 'danielgatis /\\n\\n      rembg',\n",
       " 'tensorflow /\\n\\n      models',\n",
       " 'mlflow /\\n\\n      mlflow',\n",
       " 'open-mmlab /\\n\\n      mmdetection',\n",
       " 'blakeblackshear /\\n\\n      frigate',\n",
       " 'cupy /\\n\\n      cupy',\n",
       " 'jackfrued /\\n\\n      Python-100-Days']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_repos2 = []\n",
    "for i in lista_repos:\n",
    "    lista_repos2.append(i.split('/\\n\\n')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python-World',\n",
       " 'huggingface',\n",
       " 'espnet',\n",
       " 'bregman-arie',\n",
       " 'pytest-dev',\n",
       " 'tiangolo',\n",
       " 'bhky',\n",
       " 'TachibanaYoshino',\n",
       " 'synacktiv',\n",
       " 'huggingface',\n",
       " 'netbox-community',\n",
       " 'public-apis',\n",
       " 'WZMIAOMIAO',\n",
       " 'ansible',\n",
       " 'swisskyrepo',\n",
       " 'pytorch',\n",
       " 'demisto',\n",
       " 'PyGithub',\n",
       " 'danielgatis',\n",
       " 'tensorflow',\n",
       " 'mlflow',\n",
       " 'open-mmlab',\n",
       " 'blakeblackshear',\n",
       " 'cupy',\n",
       " 'jackfrued']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_repos2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.find_all('a', {'class': 'image'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag[0].get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_links = []\n",
    "for i in range(len(tag)):\n",
    "    lista_links.append(tag[i].get(\"href\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/File:Walt_Disney_1946.JPG',\n",
       " '/wiki/File:Walt_Disney_1942_signature.svg',\n",
       " '/wiki/File:Walt_Disney_envelope_ca._1921.jpg',\n",
       " '/wiki/File:Trolley_Troubles_poster.jpg',\n",
       " '/wiki/File:Steamboat-willie.jpg',\n",
       " '/wiki/File:Walt_Disney_1935.jpg',\n",
       " '/wiki/File:Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg',\n",
       " '/wiki/File:Disney_drawing_goofy.jpg',\n",
       " '/wiki/File:DisneySchiphol1951.jpg',\n",
       " '/wiki/File:WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '/wiki/File:Walt_disney_portrait_right.jpg',\n",
       " '/wiki/File:Walt_Disney_Grave.JPG',\n",
       " '/wiki/File:Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '/wiki/File:Disney_Display_Case.JPG',\n",
       " '/wiki/File:Disney1968.jpg',\n",
       " '/wiki/File:Disneyland_Resort_logo.svg',\n",
       " '/wiki/File:Animation_disc.svg',\n",
       " '/wiki/File:P_vip.svg',\n",
       " '/wiki/File:Magic_Kingdom_castle.jpg',\n",
       " '/wiki/File:Video-x-generic.svg',\n",
       " '/wiki/File:Flag_of_Los_Angeles_County,_California.svg',\n",
       " '/wiki/File:Blank_television_set.svg',\n",
       " '/wiki/File:Flag_of_the_United_States.svg']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "elementos = soup.find_all('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/Pythonidae\n",
      "/wiki/Python_(genus)\n",
      "#Computing\n",
      "#People\n",
      "#Roller_coasters\n",
      "#Vehicles\n",
      "#Weaponry\n",
      "#Other_uses\n",
      "#See_also\n",
      "/wiki/Python_(programming_language)\n",
      "/wiki/CMU_Common_Lisp\n",
      "/wiki/PERQ#PERQ_3\n",
      "/wiki/Python_of_Aenus\n",
      "/wiki/Python_(painter)\n",
      "/wiki/Python_of_Byzantium\n",
      "/wiki/Python_of_Catana\n",
      "/wiki/Python_Anghelo\n",
      "/wiki/Python_(Efteling)\n",
      "/wiki/Python_(Busch_Gardens_Tampa_Bay)\n",
      "/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n",
      "/wiki/Python_(automobile_maker)\n",
      "/wiki/Python_(Ford_prototype)\n",
      "/wiki/Python_(missile)\n",
      "/wiki/Python_(nuclear_primary)\n",
      "/wiki/Colt_Python\n",
      "/wiki/PYTHON\n",
      "/wiki/Python_(film)\n",
      "/wiki/Python_(mythology)\n",
      "/wiki/Monty_Python\n",
      "/wiki/Python_(Monty)_Pictures\n",
      "/wiki/Cython\n",
      "/wiki/Pyton\n",
      "/wiki/Pithon\n",
      "/wiki/Category:Disambiguation_pages\n",
      "/wiki/Category:Human_name_disambiguation_pages\n",
      "/wiki/Category:Disambiguation_pages_with_given-name-holder_lists\n",
      "/wiki/Category:Disambiguation_pages_with_short_descriptions\n",
      "/wiki/Category:Short_description_is_different_from_Wikidata\n",
      "/wiki/Category:All_article_disambiguation_pages\n",
      "/wiki/Category:All_disambiguation_pages\n",
      "/wiki/Category:Animal_common_name_disambiguation_pages\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for elemento in range(len(elementos)):\n",
    "        print(elementos[elemento].find(\"a\").get(\"href\"))\n",
    "except: 'NoneType'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = soup.find_all('div', {'class': 'usctitlechanged'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Latitud</th>\n",
       "      <th>Longitud</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>18:19:27.011</td>\n",
       "      <td>7.03</td>\n",
       "      <td>108.08</td>\n",
       "      <td>JAVA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>18:18:27.012</td>\n",
       "      <td>28.55</td>\n",
       "      <td>17.83</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>18:16:02.615</td>\n",
       "      <td>19.25</td>\n",
       "      <td>155.43</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>18:06:36.624</td>\n",
       "      <td>28.58</td>\n",
       "      <td>17.81</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>17:46:57.044</td>\n",
       "      <td>22.71</td>\n",
       "      <td>68.54</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>17:40:31.050</td>\n",
       "      <td>10.98</td>\n",
       "      <td>86.56</td>\n",
       "      <td>OFF COAST OF COSTA RICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>17:28:58.91hr 02</td>\n",
       "      <td>19.25</td>\n",
       "      <td>155.40</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>17:24:21.01hr 06</td>\n",
       "      <td>1.92</td>\n",
       "      <td>122.50</td>\n",
       "      <td>SULAWESI, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>17:15:43.01hr 15</td>\n",
       "      <td>8.36</td>\n",
       "      <td>110.01</td>\n",
       "      <td>JAVA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>17:13:22.61hr 17</td>\n",
       "      <td>37.29</td>\n",
       "      <td>36.97</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>17:10:55.41hr 20</td>\n",
       "      <td>19.19</td>\n",
       "      <td>155.47</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>16:53:21.01hr 37</td>\n",
       "      <td>36.89</td>\n",
       "      <td>24.25</td>\n",
       "      <td>SOUTHERN GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>16:22:13.02hr 08</td>\n",
       "      <td>0.56</td>\n",
       "      <td>121.18</td>\n",
       "      <td>MINAHASA, SULAWESI, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>16:16:54.82hr 14</td>\n",
       "      <td>38.87</td>\n",
       "      <td>31.08</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>15:59:14.42hr 31</td>\n",
       "      <td>38.73</td>\n",
       "      <td>27.81</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>15:59:06.02hr 31</td>\n",
       "      <td>21.77</td>\n",
       "      <td>68.68</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>15:58:34.02hr 32</td>\n",
       "      <td>12.44</td>\n",
       "      <td>87.94</td>\n",
       "      <td>NEAR COAST OF NICARAGUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>15:51:58.02hr 39</td>\n",
       "      <td>8.91</td>\n",
       "      <td>124.12</td>\n",
       "      <td>KEPULAUAN ALOR, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>15:41:14.02hr 49</td>\n",
       "      <td>1.65</td>\n",
       "      <td>127.30</td>\n",
       "      <td>KEPULAUAN OBI, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>15:34:57.92hr 56</td>\n",
       "      <td>28.55</td>\n",
       "      <td>17.84</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date              Time Latitud Longitud  \\\n",
       "0   2021-11-11      18:19:27.011    7.03   108.08   \n",
       "1   2021-11-11      18:18:27.012   28.55    17.83   \n",
       "2   2021-11-11      18:16:02.615   19.25   155.43   \n",
       "3   2021-11-11      18:06:36.624   28.58    17.81   \n",
       "4   2021-11-11      17:46:57.044   22.71    68.54   \n",
       "5   2021-11-11      17:40:31.050   10.98    86.56   \n",
       "6   2021-11-11  17:28:58.91hr 02   19.25   155.40   \n",
       "7   2021-11-11  17:24:21.01hr 06    1.92   122.50   \n",
       "8   2021-11-11  17:15:43.01hr 15    8.36   110.01   \n",
       "9   2021-11-11  17:13:22.61hr 17   37.29    36.97   \n",
       "10  2021-11-11  17:10:55.41hr 20   19.19   155.47   \n",
       "11  2021-11-11  16:53:21.01hr 37   36.89    24.25   \n",
       "12  2021-11-11  16:22:13.02hr 08    0.56   121.18   \n",
       "13  2021-11-11  16:16:54.82hr 14   38.87    31.08   \n",
       "14  2021-11-11  15:59:14.42hr 31   38.73    27.81   \n",
       "15  2021-11-11  15:59:06.02hr 31   21.77    68.68   \n",
       "16  2021-11-11  15:58:34.02hr 32   12.44    87.94   \n",
       "17  2021-11-11  15:51:58.02hr 39    8.91   124.12   \n",
       "18  2021-11-11  15:41:14.02hr 49    1.65   127.30   \n",
       "19  2021-11-11  15:34:57.92hr 56   28.55    17.84   \n",
       "\n",
       "                           Region  \n",
       "0                 JAVA, INDONESIA  \n",
       "1    CANARY ISLANDS, SPAIN REGION  \n",
       "2        ISLAND OF HAWAII, HAWAII  \n",
       "3    CANARY ISLANDS, SPAIN REGION  \n",
       "4              ANTOFAGASTA, CHILE  \n",
       "5         OFF COAST OF COSTA RICA  \n",
       "6        ISLAND OF HAWAII, HAWAII  \n",
       "7             SULAWESI, INDONESIA  \n",
       "8                 JAVA, INDONESIA  \n",
       "9                  CENTRAL TURKEY  \n",
       "10       ISLAND OF HAWAII, HAWAII  \n",
       "11                SOUTHERN GREECE  \n",
       "12  MINAHASA, SULAWESI, INDONESIA  \n",
       "13                 WESTERN TURKEY  \n",
       "14                 WESTERN TURKEY  \n",
       "15             ANTOFAGASTA, CHILE  \n",
       "16        NEAR COAST OF NICARAGUA  \n",
       "17      KEPULAUAN ALOR, INDONESIA  \n",
       "18       KEPULAUAN OBI, INDONESIA  \n",
       "19   CANARY ISLANDS, SPAIN REGION  "
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tabla= soup.findAll('tr')\n",
    "Earthquakes=Tabla[14:34]\n",
    "ListaDeEarthquake=[]\n",
    "for i in range(len(Earthquakes)):\n",
    "    Diccionario={\"Date\": Earthquakes[i].findAll('td')[3].getText().strip().split('\\xa0\\xa0\\xa0')[0].split('ke')[1],\n",
    "                \"Time\":Earthquakes[i].findAll('td')[3].getText().strip().split('\\xa0\\xa0\\xa0')[1].split('min')[0],\n",
    "                \"Latitud\":Earthquakes[i].findAll('td')[4].getText().strip(),\n",
    "                \"Longitud\": Earthquakes[i].findAll('td')[6].getText().strip(),\n",
    "                \"Region\":Earthquakes[i].findAll('td')[11].getText().strip()}\n",
    "    ListaDeEarthquake.append(Diccionario)\n",
    "Terremotos=pd.DataFrame(ListaDeEarthquake)\n",
    "Terremotos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'\n",
    "url_hack = 'https://hackevents.co/search/anything/anywhere/anytime' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "NO HACERLO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "idiomas = soup.find_all(\"div\",{\"class\":\"central-featured-lang\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista=[]\n",
    "for elemento in range(len(idiomas)):\n",
    "    lista.append(idiomas[elemento].getText().strip().split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English 6383000+\n",
      "日本語 1292000+\n",
      "Русский 1756000+\n",
      "Deutsch 2617000+\n",
      "Español 1717000+\n",
      "Français 2362000+\n",
      "中文 1231000+\n",
      "Italiano 1718000+\n",
      "Português 1074000+\n",
      "Polski 1490000+\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "\n",
    "for i in range(len(lista)):\n",
    "        print(lista[i], \"\".join(idiomas[i].getText().strip().split()[1:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/search?filters%5Btopic%5D=Health'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code \n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = soup.find_all(\"a\",{\"class\":\"govuk-link\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"govuk-link\" href=\"/cookies\">cookies to collect information</a>,\n",
       " <a class=\"govuk-link\" href=\"/cookies\">View cookies</a>,\n",
       " <a class=\"govuk-link\" data-module=\"gem-track-click\" data-track-action=\"Cookie banner settings clicked from confirmation\" data-track-category=\"cookieBanner\" href=\"/cookies\">change your cookie settings</a>,\n",
       " <a class=\"govuk-link\" href=\"http://www.smartsurvey.co.uk/s/3SEXD/\">feedback</a>,\n",
       " <a class=\"govuk-link\" href=\"/search?q=\">Remove filters</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/30c8da01-d90a-44c3-ba0f-ee1d1179b262/medical-tests\">Medical Tests</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/9eb3409e-6377-48ee-9e76-ef15a27640f5/annual-report-of-incidents-food-standards-agency\">Annual report of incidents - Food Standards Agency</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/a645e5dc-d8bd-4acc-8f90-565d29b43319/uk-food-hygiene-rating-data-yorkshire-and-humberside-food-standards-agency\">UK food hygiene rating data (Yorkshire and Humberside) - Food Standards Agency</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/b4ec2594-1d52-4312-a7be-fd7b08e62c5d/uk-food-hygiene-rating-data-scotland-food-standards-agency\">UK food hygiene rating data (Scotland) - Food Standards Agency</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/b7a51800-b36f-4878-b017-2d4c0862e95f/fsa-it-health-checks\">FSA IT Health Checks</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/b9c87a1f-0c6a-479a-8fef-f85606b296fc/uk-food-hygiene-rating-data-northern-ireland-food-standards-agency\">UK food hygiene rating data (Northern Ireland) - Food Standards Agency</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/c4047886-c3ba-427f-bafc-9804c11a5ab9/food-hygiene-ratings-website-pages\">Food Hygiene Ratings website pages</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/bee79b5d-0bc4-430f-88ed-cedd3c0aac7e/uk-food-hygiene-rating-data-wales-welsh-language-food-standards-agency\">UK food hygiene rating data (Wales-Welsh-language) - Food Standards Agency</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/c7180071-3ee3-4d41-ad67-7120768ab61e/national-diet-and-nutrition-survey-scotland\">National Diet and Nutrition Survey (Scotland)</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/d22790e9-f2b1-43a6-a1ba-7c9bd4e8e4fc/health-in-pregnancy-grant-hipg\">Health in Pregnancy Grant (HiPG)</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/d4988f8e-0862-4e2f-afc4-9dad8c7f7879/uk-food-hygiene-rating-data-west-midlands-food-standards-agency\">UK food hygiene rating data (West Midlands) - Food Standards Agency</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/da412812-ba2a-4163-adf2-c94c2c3eae3c/health-in-pregnancy-grant-hipg\">Health in Pregnancy Grant (HiPG)</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/e2e05aaf-5edd-4188-b243-69ae239f91c3/health-and-safety-incidents\">Health and Safety Incidents</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/e945d1f5-72fe-4b08-8b58-8e68d83441b5/uk-food-hygiene-rating-data-london-food-standards-agency\">UK food hygiene rating data (London) - Food Standards Agency</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/4abefa87-d891-42ba-946a-c7533ee347cb/uk-food-hygiene-rating-data-north-east-food-standards-agency\">UK food hygiene rating data (North East) - Food Standards Agency</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/6428f801-8d29-4ad6-8414-192459baec63/chalara-fraxinea-infection-sites\">Chalara Fraxinea Infection Sites</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/669273c6-13df-445c-963d-50e1f9419c5f/uk-food-hygiene-rating-data-wales-english-language-food-standards-agency\">UK food hygiene rating data (Wales - English language) - Food Standards Agency</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/7bd91ab0-2fc2-479b-9597-8d96158ce4d1/uk-food-hygiene-rating-data-south-west-food-standards-agency\">UK food hygiene rating data (South West) - Food Standards Agency</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/4711413d-9504-4dca-a63c-698d0bd6af5e/uk-food-hygiene-rating-data-north-west-food-standards-agency\">UK food hygiene rating data (North West) - Food Standards Agency</a>,\n",
       " <a class=\"govuk-link\" href=\"/dataset/47f4970b-4dd0-4a5f-8c1a-ba4b40cfd3ad/health-and-safety-incidents\">Health and Safety Incidents</a>]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data2=[]\n",
    "    for i in range(len(datasets)):\n",
    "        data2.append(datasets[i].getText())\n",
    "except: 'NoneType'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data2[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Medical Tests',\n",
       " 'Annual report of incidents - Food Standards Agency',\n",
       " 'UK food hygiene rating data (Yorkshire and Humberside) - Food Standards Agency',\n",
       " 'UK food hygiene rating data (Scotland) - Food Standards Agency',\n",
       " 'FSA IT Health Checks',\n",
       " 'UK food hygiene rating data (Northern Ireland) - Food Standards Agency',\n",
       " 'Food Hygiene Ratings website pages',\n",
       " 'UK food hygiene rating data (Wales-Welsh-language) - Food Standards Agency',\n",
       " 'National Diet and Nutrition Survey (Scotland)',\n",
       " 'Health in Pregnancy Grant (HiPG)',\n",
       " 'UK food hygiene rating data (West Midlands) - Food Standards Agency',\n",
       " 'Health in Pregnancy Grant (HiPG)',\n",
       " 'Health and Safety Incidents',\n",
       " 'UK food hygiene rating data (London) - Food Standards Agency',\n",
       " 'UK food hygiene rating data (North East) - Food Standards Agency',\n",
       " 'Chalara Fraxinea Infection Sites',\n",
       " 'UK food hygiene rating data (Wales - English language) - Food Standards Agency',\n",
       " 'UK food hygiene rating data (South West) - Food Standards Agency',\n",
       " 'UK food hygiene rating data (North West) - Food Standards Agency',\n",
       " 'Health and Safety Incidents']"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers(millions)</th>\n",
       "      <th>Percentageof world pop.(March 2019)</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918</td>\n",
       "      <td>11.922%</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "      <td>5.994%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "      <td>4.922%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hindi (sanskritised Hindustani)</td>\n",
       "      <td>341</td>\n",
       "      <td>4.429%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>300</td>\n",
       "      <td>4.000%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221</td>\n",
       "      <td>2.870%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "      <td>2.000%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>128</td>\n",
       "      <td>1.662%</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Western Punjabi[</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.204%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "      <td>1.079%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank                         Language Speakers(millions)  \\\n",
       "0    1                 Mandarin Chinese                918   \n",
       "1    2                          Spanish                480   \n",
       "2    3                          English                379   \n",
       "3    4  Hindi (sanskritised Hindustani)                341   \n",
       "4    5                          Bengali                300   \n",
       "5    6                       Portuguese                221   \n",
       "6    7                          Russian                154   \n",
       "7    8                         Japanese                128   \n",
       "8    9                 Western Punjabi[               92.7   \n",
       "9   10                          Marathi               83.1   \n",
       "\n",
       "  Percentageof world pop.(March 2019) Language family        Branch  \n",
       "0                             11.922%    Sino-Tibetan       Sinitic  \n",
       "1                              5.994%   Indo-European       Romance  \n",
       "2                              4.922%   Indo-European      Germanic  \n",
       "3                              4.429%   Indo-European    Indo-Aryan  \n",
       "4                              4.000%   Indo-European    Indo-Aryan  \n",
       "5                              2.870%   Indo-European       Romance  \n",
       "6                              2.000%   Indo-European  Balto-Slavic  \n",
       "7                              1.662%         Japonic      Japanese  \n",
       "8                              1.204%   Indo-European    Indo-Aryan  \n",
       "9                              1.079%   Indo-European    Indo-Aryan  "
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = soup.find_all('table', attrs = {'class':'wikitable'})[0]\n",
    "rows = table.find_all('td')\n",
    "columns = table.find_all('th')\n",
    "language_table = [tag.text.replace('\\n', '')[:-3] if '[' in tag.text.replace('\\n', '') else tag.text.replace('\\n', '') for tag in rows]\n",
    "columns = [tag.text.replace('\\n', '')[:-3] if '[' in tag.text.replace('\\n', '') else tag.text.replace('\\n', '') for tag in columns]\n",
    "languages_dict = {columns[i]:language_table[i::len(columns)] for i in range(len(columns))}\n",
    "languages_df = pd.DataFrame(languages_dict)\n",
    "languages_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = soup.find_all(\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [ tags[i].find_all(\"td\")[1].find(\"a\").get(\"href\")  for i in range(1,len(tags))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = [tags[i].find_all(\"td\")[1].find(\"a\").get(\"title\")  for i in range(1,len(tags))]\n",
    "# stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [list(tags[i].find_all(\"td\")[1].find(\"a\"))  for i in range(1,len(tags))]\n",
    "# titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(soup.find_all(\"span\", {\"class\": \"secondaryInfo\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_ok=[]\n",
    "for a in years:\n",
    "    years_ok.append(int(a.getText().split(\"(\")[1].split(\")\")[0]))\n",
    "# years_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=[[titles[i], years_ok[i], stars[i].split(\"(dir.)\")[0], stars[i].split(\"(dir.)\")[1] ] for  i in range(1,len(titles)) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[El padrino]</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>, Marlon Brando, Al Pacino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[El padrino: Parte II]</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>, Al Pacino, Robert De Niro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[El caballero oscuro]</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>, Christian Bale, Heath Ledger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[12 hombres sin piedad]</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>, Henry Fonda, Lee J. Cobb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[La lista de Schindler]</td>\n",
       "      <td>1993</td>\n",
       "      <td>Steven Spielberg</td>\n",
       "      <td>, Liam Neeson, Ralph Fiennes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>[La princesa prometida]</td>\n",
       "      <td>1987</td>\n",
       "      <td>Rob Reiner</td>\n",
       "      <td>, Cary Elwes, Mandy Patinkin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>[Las noches de Cabiria]</td>\n",
       "      <td>1957</td>\n",
       "      <td>Federico Fellini</td>\n",
       "      <td>, Giulietta Masina, François Périer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>[Paris, Texas]</td>\n",
       "      <td>1984</td>\n",
       "      <td>Wim Wenders</td>\n",
       "      <td>, Harry Dean Stanton, Nastassja Kinski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>[Tres colores: Rojo]</td>\n",
       "      <td>1994</td>\n",
       "      <td>Krzysztof Kieslowski</td>\n",
       "      <td>, Irène Jacob, Jean-Louis Trintignant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>[Sardar Udham]</td>\n",
       "      <td>2021</td>\n",
       "      <td>Shoojit Sircar</td>\n",
       "      <td>, Vicky Kaushal, Banita Sandhu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0     1                      2  \\\n",
       "0               [El padrino]  1972  Francis Ford Coppola    \n",
       "1     [El padrino: Parte II]  1974  Francis Ford Coppola    \n",
       "2      [El caballero oscuro]  2008     Christopher Nolan    \n",
       "3    [12 hombres sin piedad]  1957          Sidney Lumet    \n",
       "4    [La lista de Schindler]  1993      Steven Spielberg    \n",
       "..                       ...   ...                    ...   \n",
       "244  [La princesa prometida]  1987            Rob Reiner    \n",
       "245  [Las noches de Cabiria]  1957      Federico Fellini    \n",
       "246           [Paris, Texas]  1984           Wim Wenders    \n",
       "247     [Tres colores: Rojo]  1994  Krzysztof Kieslowski    \n",
       "248           [Sardar Udham]  2021        Shoojit Sircar    \n",
       "\n",
       "                                          3  \n",
       "0                , Marlon Brando, Al Pacino  \n",
       "1               , Al Pacino, Robert De Niro  \n",
       "2            , Christian Bale, Heath Ledger  \n",
       "3                , Henry Fonda, Lee J. Cobb  \n",
       "4              , Liam Neeson, Ralph Fiennes  \n",
       "..                                      ...  \n",
       "244            , Cary Elwes, Mandy Patinkin  \n",
       "245     , Giulietta Masina, François Périer  \n",
       "246  , Harry Dean Stanton, Nastassja Kinski  \n",
       "247   , Irène Jacob, Jean-Louis Trintignant  \n",
       "248          , Vicky Kaushal, Banita Sandhu  \n",
       "\n",
       "[249 rows x 4 columns]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(final)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
